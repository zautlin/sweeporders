╔════════════════════════════════════════════════════════════════════════════════════╗
║                    SYSTEM ARCHITECTURE OVERVIEW                                   ║
║                 Chunk-Based Parallel Processing Pipeline                          ║
╚════════════════════════════════════════════════════════════════════════════════════╝


CURRENT SYSTEM (Working ✓)
════════════════════════════════════════════════════════════════════════════════════

                    ┌─────────────────────┐
                    │  Raw Data Files     │
                    │  (48K orders)       │
                    │  (8K trades)        │
                    │  (2 NBBO snapshots) │
                    └──────────┬──────────┘
                               │
                    ┌──────────▼──────────┐
                    │   STEP 1: INGEST    │
                    │  (Filter by date &  │
                    │   participant)      │
                    └──────────┬──────────┘
                               │
                    ┌──────────▼──────────┐
                    │  STEP 2: CLASSIFY   │
                    │  (Group 1,2,3)      │
                    └──────────┬──────────┘
                               │
     ┌─────────────────────────┼─────────────────────────┐
     │                         │                         │
  ┌──▼────┐             ┌──────▼──────┐          ┌──────▼──────┐
  │STEP 4:│             │STEP 6:      │          │STEP 7:      │
  │REAL   │             │SIMULATION   │          │EXTENDED     │
  │METRICS│             │(3 scenarios)│          │ANALYSIS     │
  └──┬────┘             └──────┬──────┘          └──────┬──────┘
     │                        │                        │
     └────────────────┬───────┴────────────┬──────────┘
                      │                    │
                   ┌──▼────────────────────▼──┐
                   │  STEP 8: STATISTICS      │
                   │  (T-tests, ANOVA)        │
                   └──┬────────────────────────┘
                      │
                   ┌──▼──────────────────┐
                   │  FINAL REPORTS      │
                   │  & METRICS          │
                   └─────────────────────┘

Execution Time: ~15 seconds
Memory: ~1GB peak
Data: Single date, single security code


PROPOSED SYSTEM (Scalable ✓)
════════════════════════════════════════════════════════════════════════════════════

                    ┌─────────────────────────────┐
                    │   RAW DATA FILES (200GB+)   │
                    │   • 2B orders               │
                    │   • 1B trades               │
                    │   • Multiple dates          │
                    │   • Multiple securities     │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────▼──────────────┐
                    │   CONFIGURATION LAYER       │
                    │   (Phase 1)                 │
                    │   security_codes: [101,102] │
                    │   date_range: [2024-01-01,  │
                    │              2024-12-31]    │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────▼──────────────┐
                    │   CHUNK ITERATOR (Phase 2)  │
                    │   • Read 1GB chunks         │
                    │   • Parse & validate        │
                    │   • Extract metadata        │
                    │   Memory: 2-3GB peak        │
                    └──────────────┬──────────────┘
                                   │
                    ┌──────────────▼──────────────┐
                    │   JOB SCHEDULER (Phase 3)   │
                    │   Generate (sec,date) jobs: │
                    │   (101,2024-01-01)          │
                    │   (101,2024-01-02)          │
                    │   (102,2024-01-01)          │
                    │   ...                       │
                    └──────────────┬──────────────┘
                                   │
         ┌─────────────────────────┼─────────────────────────┐
         │                         │                         │
    ┌────▼────┐            ┌──────▼──────┐        ┌──────────▼────┐
    │ Worker 1│            │ Worker 2    │   ...  │ Worker 8      │
    ├─────────┤            ├─────────────┤        ├───────────────┤
    │SEC_101_ │            │SEC_101_     │        │SEC_102_       │
    │2024-01- │            │2024-01-02   │        │2024-01-01     │
    │01       │            │             │        │               │
    │         │            │             │        │               │
    │Step 1:  │            │Step 1:      │        │Step 1:        │
    │Ingest   │            │Ingest       │        │Ingest         │
    │(chunk)  │            │(chunk)      │        │(chunk)        │
    │         │            │             │        │               │
    │Step 2:  │            │Step 2:      │        │Step 2:        │
    │Classify │            │Classify     │        │Classify       │
    │         │            │             │        │               │
    │Step 4:  │            │Step 4:      │        │Step 4:        │
    │Metrics  │            │Metrics      │        │Metrics        │
    │         │            │             │        │               │
    │Step 6:  │            │Step 6:      │        │Step 6:        │
    │Simulate │            │Simulate     │        │Simulate       │
    │         │            │             │        │               │
    │Output:  │            │Output:      │        │Output:        │
    │results/ │            │results/     │        │results/       │
    │sec101/  │            │sec101/      │        │sec102/        │
    │2024-01- │            │2024-01-02/  │        │2024-01-01/    │
    │01/      │            │             │        │               │
    └────┬────┘            └──────┬──────┘        └────────┬──────┘
         │                        │                       │
         └────────────────┬───────┴───────────┬──────────┘
                          │                   │
                 ┌────────▼───────────────────▼────────┐
                 │   AGGREGATION LAYER (Phase 5)       │
                 │   • Load all per-(sec,date) results │
                 │   • Aggregate by security           │
                 │   • Aggregate by date               │
                 │   • Aggregate by participant        │
                 │   • Generate global metrics         │
                 └────────────┬────────────────────────┘
                              │
                 ┌────────────▼─────────────┐
                 │   AGGREGATED RESULTS    │
                 │   • global_summary.csv   │
                 │   • by_security.csv      │
                 │   • by_date.csv          │
                 │   • by_participant.csv   │
                 │   • time_series.csv      │
                 └──────────────────────────┘

Execution Time: ~25-30 hours (vs 200+ hours sequential)
Memory: 20-24GB peak (8 workers × 2-3GB each)
Data: Multiple dates, multiple securities, parallel processing


CORE COMPONENTS
════════════════════════════════════════════════════════════════════════════════════

1. Configuration Layer
   └─ File: config/scaling_config.py
      ├─ Define security codes
      ├─ Date ranges
      ├─ Chunk size (1GB default)
      ├─ Worker count (8 default)
      └─ Output format

2. Chunk Iterator
   └─ File: src/chunk_iterator.py
      ├─ Read file in 1GB chunks
      ├─ Parse without loading entire file
      ├─ Extract metadata
      └─ Handle row boundaries

3. Job Scheduler
   └─ File: src/parallel_scheduler.py
      ├─ Generate job matrix
      ├─ Submit to ProcessPoolExecutor
      ├─ Monitor execution
      └─ Collect results

4. Refactored Ingestion
   └─ File: src/ingest_chunked.py
      ├─ Uses chunk iterator
      ├─ Dynamic filtering
      ├─ Steps 1, 2, 4, 6 integrated
      └─ Per-(security, date) extraction

5. Result Aggregation
   └─ File: src/result_aggregator.py
      ├─ Load all job results
      ├─ Aggregate by dimension
      ├─ Generate summary metrics
      └─ Export consolidated results

6. Monitoring & Logging
   └─ File: src/job_monitor.py
      ├─ Real-time progress tracking
      ├─ ETA calculation
      ├─ Performance statistics
      └─ Execution summary


DATA FLOW DIAGRAM
════════════════════════════════════════════════════════════════════════════════════

Config
  │
  ├─ securities: [101, 102]
  ├─ dates: [2024-01-01 to 2024-01-05]
  └─ chunk_size: 1GB
       │
       ▼
Job Matrix Generator
  │
  └─ Creates 10 jobs:
     (101, 2024-01-01) → Job 1
     (101, 2024-01-02) → Job 2
     (101, 2024-01-03) → Job 3
     (101, 2024-01-04) → Job 4
     (101, 2024-01-05) → Job 5
     (102, 2024-01-01) → Job 6
     (102, 2024-01-02) → Job 7
     (102, 2024-01-03) → Job 8
     (102, 2024-01-04) → Job 9
     (102, 2024-01-05) → Job 10
       │
       ▼
Parallel Job Executor (8 workers)
  │
  ├─ Worker 1: Processing Job 1
  ├─ Worker 2: Processing Job 2
  ├─ Worker 3: Processing Job 3
  ├─ Worker 4: Processing Job 4
  ├─ Worker 5: Processing Job 5
  ├─ Worker 6: Processing Job 6
  ├─ Worker 7: Processing Job 7
  └─ Worker 8: Processing Job 8
       │
       │ (When Job 1 finishes, Job 9 starts)
       │ (When Job 2 finishes, Job 10 starts)
       │
       ▼
Result Collection
  │
  ├─ results/sec101/2024-01-01/metrics.csv
  ├─ results/sec101/2024-01-02/metrics.csv
  ├─ ...
  └─ results/sec102/2024-01-05/metrics.csv
       │
       ▼
Aggregation
  │
  ├─ by_security.csv (summarize all dates for each security)
  ├─ by_date.csv (summarize all securities for each date)
  ├─ by_participant.csv (global participant metrics)
  └─ global_summary.csv (overall metrics)


PERFORMANCE SCALING
════════════════════════════════════════════════════════════════════════════════════

File Size vs Processing Time vs Memory

100GB file with 4 securities × 90 days = 360 jobs:

Workers     Time        Memory           Notes
────────────────────────────────────────────────────
1           180 hours   2-3GB            Sequential (too slow)
2           90 hours    4-6GB            Still slow
4           45 hours    8-12GB           Better
8           22-24 hours 16-24GB          Good (practical)
16          11-12 hours 32-48GB          Excellent (if hardware available)

Speedup is nearly linear: 8 workers = ~8x faster


EXAMPLE CONFIGURATION (config/scaling_config.py)
════════════════════════════════════════════════════════════════════════════════════

processing:
  mode: 'parallel'
  max_workers: 8
  chunk_size_mb: 1024
  temp_dir: 'temp_chunks/'

data_selection:
  security_codes: [101, 102, 103]
  date_range:
    start: '2024-01-01'
    end: '2024-12-31'
    all_dates: false
  participant_ids: [69]
  trading_hours:
    start: 10
    end: 16

simulation:
  dark_pool_scenarios: ['A', 'B', 'C']
  price_impact_percent: 0.05

output:
  format: 'gzip'
  aggregate_by:
    - 'security_code'
    - 'date'
    - 'participant_id'
  detailed_logs: true


IMPLEMENTATION PHASES SUMMARY
════════════════════════════════════════════════════════════════════════════════════

Phase 1 (2h):   Configuration + job matrix generation
Phase 2 (3h):   Chunk iterator for streaming large files
Phase 3 (4h):   Parallel job scheduler with multiprocessing
Phase 4 (3h):   Refactored ingest using chunk iterator
Phase 5 (3h):   Result aggregation across all dimensions
Phase 6 (2h):   Monitoring and progress tracking
Phase 7 (4h):   Testing with synthetic data
Phase 8 (4h):   Performance benchmarking and optimization

Total: 25 hours (1-2 weeks)


KEY METRICS
════════════════════════════════════════════════════════════════════════════════════

Current (Single date, single security):
  Orders: 156
  Trades: 60
  Time: 15 seconds
  Memory: 1GB

Projected (Multiple dates, multiple securities, parallel):
  Orders: 2,000,000,000+ (with 200GB file)
  Trades: 1,000,000,000+ (with 200GB file)
  Time: 25-30 hours (8 workers)
  Memory: 20-24GB peak (distributed)
  Throughput: ~80,000 orders/second (aggregate)


═══════════════════════════════════════════════════════════════════════════════════
Ready to scale from 50K to billions of orders with memory-efficient parallel processing
═══════════════════════════════════════════════════════════════════════════════════
